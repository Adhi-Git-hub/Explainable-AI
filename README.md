# Explainable-AI

## What is Explainable AI?

Explainable AI (XAI) refers to methods and techniques in artificial intelligence that make the outputs of AI models understandable to humans. As AI systems are increasingly deployed in various applications, it is crucial to interpret their decisions, especially in high-stakes domains such as healthcare, finance, and law. XAI aims to provide insights into how models make predictions, enabling users to trust and effectively utilize AI systems. Techniques like LIME (Local Interpretable Model-agnostic Explanations) help in interpreting complex models by approximating them locally with interpretable models.

 ![sample image]( [https://github.com/Adhi-Git-hub/Neural-Style-Transfer/blob/main/TensorFlow%20Hub%20Model-1.jpg](https://github.com/Adhi-Git-hub/Explainable-AI/blob/main/Screenshot%202024-09-16%20233755.png))

## About the Codes

This repository contains three Python files that demonstrate the application of Explainable AI using different datasets and machine learning models:

1. **LeNet Model for MNIST Image Dataset**
   - This code implements a deep learning model (LeNet) designed for classifying handwritten digits from the MNIST dataset. It also utilizes LIME to explain the predictions made by the model, providing insights into which features contributed to the classification of each digit.

2. **Iris Numerical Dataset Model**
   - This file contains a machine learning model developed using the Iris dataset, which is a classic dataset in pattern recognition. The model predicts the species of iris plants based on their features. LIME is employed to interpret the predictions and understand the influence of various features on the model's decisions.

3. **Text Dataset for Abusive Comment Detection**
   - This code focuses on detecting abusive comments in textual data using a Bidirectional LSTM model. The model classifies comments as abusive or non-abusive. LIME is used to explain the model's predictions by highlighting the words or phrases in the comments that significantly impacted the classification outcome.


[Link Text]([relative/path/to/your/file.pdf](https://github.com/Adhi-Git-hub/Explainable-AI/blob/main/Explainable%20AI.pdf))

